% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lightgbm.R
\name{lgbm_train}
\alias{lgbm_train}
\title{Boosted trees via lightgbm}
\usage{
lgbm_train(
  x,
  y,
  max_depth = 6,
  nrounds = 15,
  learning_rate = 0.3,
  feature_fraction = 1,
  min_data_in_leaf = 20,
  bagging_fraction = 1,
  min_gain_to_split = 0,
  validation = 0,
  early_stop = NULL,
  ...
)
}
\arguments{
\item{x}{A data frame or matrix of predictors}

\item{y}{A vector (factor or numeric) or matrix (numeric) of outcome data.}

\item{max_depth}{An integer for the maximum depth of the tree.}

\item{nrounds}{An integer for the number of boosting iterations.}

\item{learning_rate}{A numeric value between zero and one to control the learning rate.}

\item{feature_fraction}{Subsampling proportion of columns.}

\item{min_data_in_leaf}{A numeric value for the minimum sum of instance
weights needed in a child to continue to split.}

\item{bagging_fraction}{Subsampling proportion of rows.}

\item{min_gain_to_split}{A number for the minimum loss reduction required to make a
further partition on a leaf node of the tree}

\item{validation}{A positive number. If on `[0, 1)` the value, `validation`
is a random proportion of data in `x` and `y` that are used for performance
assessment and potential early stopping. If 1 or greater, it is the _number_
of training set samples use for these purposes.}

\item{early_stop}{An integer or `NULL`. If not `NULL`, it is the number of
training iterations without improvement before stopping. If `validation` is
used, performance is base on the validation set; otherwise the training set
is used.}

\item{...}{Other options to pass to `lgbm.train`.}
}
\value{
A fitted `lightgbm` object.
}
\description{
`lgbm_train` is a wrapper for `xgboost` tree-based models
 where all of the model arguments are in the main function.
}
\keyword{internal}
